{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "with open('text.txt','r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "2G7OjkbX0ruu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8ncep5y1gA5",
        "outputId": "524279e2-8b42-474a-87ff-7681487c47d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "296230\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "id": "VQ9EERwn2jvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLBGXnCO20bn",
        "outputId": "c76d6706-f86c-4c92-88bb-1fbee2083ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~Â Â¡Â£Â¬Â¯Â°Â²Â³Â´Â¹Ã Ã¡Ã¥Ã©Ã­Ã³Ã¸ÃºÃ¼Ä€ÄÄ¡ÄªÄ«Å‚Å„Å›Å«ÉªÉ´Ê€ÊÊ–Ê™ÊœÊŸÊ»Ê¾Ê¿Ë¹ËºÍœÍŸÍ Í¡Ï€ÏƒÏ†Ï‰ØŒØŸØ¡Ø¢Ø£Ø¤Ø¥Ø¦Ø§Ø¨Ø©ØªØ«Ø¬Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙ€ÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙ‰ÙŠÙ‹ÙŒÙÙÙÙÙ‘Ù’Ù¡Ù¢Ù£Ù¤Ù¥Ù§Ù©Ù°Ù±ÛÛŒÛ—ÛšÛ¡à² á´€á´…á´‡á´‹á´›á´œá´¡á´¢á¸¤á¸¥á¹¬á¹­â€Šâ€‹â€Œâ€â€â€â€“â€”â€˜â€™â€œâ€â€¢â€¦â€²â€µâ€¼â âµâ‚¹âƒ£â†’â†—â‰¦â‰§â‹ˆâŒâ•¯â•°â–€â–„â–ˆâ–‘â–’â– â–¡â–½â—‹â—â—•â—¡â˜â˜ â˜¹â™€â™‚â™¡â™ªâš âš¡âœ‚âœ…âœŠâœ”âœ¦âœ§âœ¨âœ³ââ‡âŒâ“âââ¡â¨ã€€ã£ãƒ„ãƒãƒ®ãƒ¾çš¿ï·ºï·»ï·½ï¸ï¸¶ï¼ˆï¼‰ï¼¾ï½ï½¥ï¾‰ï¾Ÿï¿£ğŸ‡¦ğŸ‡§ğŸ‡¨ğŸ‡©ğŸ‡ªğŸ‡«ğŸ‡¬ğŸ‡­ğŸ‡®ğŸ‡¯ğŸ‡°ğŸ‡±ğŸ‡²ğŸ‡³ğŸ‡´ğŸ‡µğŸ‡·ğŸ‡¸ğŸ‡¹ğŸ‡ºğŸ‡»ğŸ‡½ğŸ‡¾ğŸ‡¿ğŸŒ€ğŸŒŒğŸŒšğŸŒ«ğŸ¥ğŸ´ğŸ½ğŸƒğŸ‰ğŸŠğŸ¡ğŸ»ğŸ¼ğŸ¿ğŸ˜ğŸ™ğŸŸğŸ§ğŸ¸ğŸ¼ğŸ‘€ğŸ‘‡ğŸ‘‰ğŸ‘ŒğŸ‘ğŸ‘ğŸ‘¥ğŸ‘¨ğŸ‘¹ğŸ‘»ğŸ‘½ğŸ‘¾ğŸ’€ğŸ’ğŸ’ ğŸ’¨ğŸ’ªğŸ’«ğŸ’¯ğŸ’µğŸ“–ğŸ“šğŸ“²ğŸ”¥ğŸ”«ğŸ˜€ğŸ˜ğŸ˜‚ğŸ˜ƒğŸ˜…ğŸ˜†ğŸ˜‡ğŸ˜‰ğŸ˜‹ğŸ˜ŒğŸ˜ğŸ˜ğŸ˜ğŸ˜‘ğŸ˜’ğŸ˜”ğŸ˜•ğŸ˜–ğŸ˜ğŸ˜ğŸ˜ŸğŸ˜¡ğŸ˜¢ğŸ˜£ğŸ˜¤ğŸ˜¨ğŸ˜©ğŸ˜«ğŸ˜¬ğŸ˜­ğŸ˜®ğŸ˜°ğŸ˜³ğŸ˜´ğŸ˜µğŸ˜¶ğŸ™ğŸ™‚ğŸ™ƒğŸ™„ğŸ™ŠğŸ™‹ğŸ™ŒğŸš¨ğŸ¤ŒğŸ¤ğŸ¤“ğŸ¤”ğŸ¤–ğŸ¤™ğŸ¤¡ğŸ¤£ğŸ¤¦ğŸ¤§ğŸ¤¨ğŸ¤«ğŸ¤¬ğŸ¤®ğŸ¤¯ğŸ¤·ğŸ¤ºğŸ¤¼ğŸ¥±ğŸ¥²ğŸ¥´ğŸ¥¶ğŸ¥¸ğŸ¥¹ğŸ¥ºğŸ¦ğŸ¦ğŸ¦¯ğŸ§ğŸ§œğŸ§ ğŸ§¢ğŸ« ğŸ«¡ğŸ«£ğŸ«¥\n",
            "454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\" Bismillah\"))\n",
        "print(decode(encode(\" Bismillah\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfT-nRZC3U4z",
        "outputId": "181ae4c0-991b-4728-8a91-1079b8a543c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 35, 74, 84, 78, 74, 77, 77, 66, 73]\n",
            " Bismillah\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape,data.dtype)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHNH11o96-oS",
        "outputId": "9c3274cb-a52e-4578-a21a-8d5febf5be6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([296230]) torch.int64\n",
            "tensor([19, 16, 18, 23, 16, 19, 20, 13,  1, 18, 25, 27, 18, 25,  1, 14,  1, 46,\n",
            "        70, 84, 84, 66, 72, 70, 84,  1, 66, 79, 69,  1, 68, 66, 77, 77, 84,  1,\n",
            "        66, 83, 70,  1, 70, 79, 69, 14, 85, 80, 14, 70, 79, 69,  1, 70, 79, 68,\n",
            "        83, 90, 81, 85, 70, 69, 15,  1, 47, 80,  1, 80, 79, 70,  1, 80, 86, 85,\n",
            "        84, 74, 69, 70,  1, 80, 71,  1, 85, 73, 74, 84,  1, 68, 73, 66, 85, 13,\n",
            "         1, 79, 80, 85,  1, 70, 87, 70, 79,  1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "wnxZIRx1Pj6G",
        "outputId": "5588a075-5da6-43e7-8c54-e12758c58c88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2/16/23, 18:18 - Messages and calls are end-to-end encrypted. No one outside of this chat, not even '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "kzTtZrRlQskt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "print(train_data[:block_size])\n",
        "print(train_data[1:block_size+1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKOEq2klRSeB",
        "outputId": "ac0fca76-b1f9-4552-c81e-4ad398f759b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([19, 16, 18, 23, 16, 19, 20, 13])\n",
            "tensor([16, 18, 23, 16, 19, 20, 13,  1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(context,'=>',target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyqxiD9VTX12",
        "outputId": "dbdcea56-87f3-4fcf-e026-204f2c3ec064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([19]) => tensor(16)\n",
            "tensor([19, 16]) => tensor(18)\n",
            "tensor([19, 16, 18]) => tensor(23)\n",
            "tensor([19, 16, 18, 23]) => tensor(16)\n",
            "tensor([19, 16, 18, 23, 16]) => tensor(19)\n",
            "tensor([19, 16, 18, 23, 16, 19]) => tensor(20)\n",
            "tensor([19, 16, 18, 23, 16, 19, 20]) => tensor(13)\n",
            "tensor([19, 16, 18, 23, 16, 19, 20, 13]) => tensor(1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # ur avg batch size\n",
        "block_size = 8 # context length\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x,y\n",
        "\n",
        "xb,yb = get_batch('train')\n",
        "print('inputs')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension = 4\n",
        "  for t in range(block_size): # time dimension = 8\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b,t]\n",
        "    print(context.tolist(),'=>',target.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjS9L-S_U-E4",
        "outputId": "7b36add1-3e35-4ee3-cb3c-a652e2557422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs\n",
            "torch.Size([4, 8])\n",
            "tensor([[80, 69, 84, 13,  1, 85, 73, 70],\n",
            "        [73, 66, 69,  1, 85, 80,  1, 77],\n",
            "        [84, 70,  1, 66, 85, 86,  1, 73],\n",
            "        [84, 73, 90, 69,  0, 67, 73, 66]])\n",
            "targets\n",
            "torch.Size([4, 8])\n",
            "tensor([[69, 84, 13,  1, 85, 73, 70, 90],\n",
            "        [66, 69,  1, 85, 80,  1, 77, 70],\n",
            "        [70,  1, 66, 85, 86,  1, 73, 74],\n",
            "        [73, 90, 69,  0, 67, 73, 66, 74]])\n",
            "----\n",
            "[80] => 69\n",
            "[80, 69] => 84\n",
            "[80, 69, 84] => 13\n",
            "[80, 69, 84, 13] => 1\n",
            "[80, 69, 84, 13, 1] => 85\n",
            "[80, 69, 84, 13, 1, 85] => 73\n",
            "[80, 69, 84, 13, 1, 85, 73] => 70\n",
            "[80, 69, 84, 13, 1, 85, 73, 70] => 90\n",
            "[73] => 66\n",
            "[73, 66] => 69\n",
            "[73, 66, 69] => 1\n",
            "[73, 66, 69, 1] => 85\n",
            "[73, 66, 69, 1, 85] => 80\n",
            "[73, 66, 69, 1, 85, 80] => 1\n",
            "[73, 66, 69, 1, 85, 80, 1] => 77\n",
            "[73, 66, 69, 1, 85, 80, 1, 77] => 70\n",
            "[84] => 70\n",
            "[84, 70] => 1\n",
            "[84, 70, 1] => 66\n",
            "[84, 70, 1, 66] => 85\n",
            "[84, 70, 1, 66, 85] => 86\n",
            "[84, 70, 1, 66, 85, 86] => 1\n",
            "[84, 70, 1, 66, 85, 86, 1] => 73\n",
            "[84, 70, 1, 66, 85, 86, 1, 73] => 74\n",
            "[84] => 73\n",
            "[84, 73] => 90\n",
            "[84, 73, 90] => 69\n",
            "[84, 73, 90, 69] => 0\n",
            "[84, 73, 90, 69, 0] => 67\n",
            "[84, 73, 90, 69, 0, 67] => 73\n",
            "[84, 73, 90, 69, 0, 67, 73] => 66\n",
            "[84, 73, 90, 69, 0, 67, 73, 66] => 74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self,vocab_siz):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
        "\n",
        "  def forward(self,idx,targets=None):\n",
        "    logits = self.token_embedding_table(idx)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits,targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits,loss = self(idx)\n",
        "      logits = logits[:,-1,:]\n",
        "      probs = F.softmax(logits,dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits,loss = m(xb,yb)"
      ],
      "metadata": {
        "id": "VZ-6SCsnlBq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(logits.shape)\n",
        "print(loss)\n",
        "print(decode(m.generate(idx=torch.zeros((1,1),dtype=torch.long),\n",
        "                        max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_w_CayPlV9D",
        "outputId": "cc42c4cd-a84b-4692-df14-4977796d7545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 454])\n",
            "tensor(6.7069, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "ğŸ¤”ğŸ¥ğŸ’ $â¡ÙƒÙ°oğŸ’ØŒeğŸ™„â¡ğŸƒØ´ğŸ‡®Gjá¹­Â¬ğŸ‡µÙğŸ˜âš ğŸ‡­â€¼Ø£âœ§ğŸ˜°ãƒ¾Å„ÊŸâ– âœ‚Ã³ğŸ§œá´‡ğŸ˜«ğŸ‘½sğŸ˜”Ã ğŸ¤«ÉªE%ğŸ˜–;Äª6â†’ğŸ‡¸&ï½¥Ù°Ù†âœ¨âš ğŸ¦¯vkğŸ‘Hâ€¢â†—â€ğŸ™âœ¦ğŸ¤º)Ù‚YğŸ˜¨á´¢â–ˆ9ï·»ğŸ˜Ù‹Ø¦ğŸŠğŸ‡©ÙğŸ¤mÙ§}ÙŠÙ¢á¹¬\n",
            "Ï‰â˜ ãƒX/FğŸ‘»ğŸ‡¸ğŸ˜Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azAaVkFUpIE6",
        "outputId": "269f78f4-45fe-4f5b-a30e-531a74012205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1996,  0.9212, -1.8676,  ...,  2.2121,  0.0315,  1.2247],\n",
            "        [ 0.5694, -0.5439,  0.6324,  ...,  0.2597, -0.1106,  1.4783],\n",
            "        [-0.6176,  0.8322, -0.7198,  ..., -0.2902,  1.5161, -0.8732],\n",
            "        ...,\n",
            "        [-1.4170, -0.3118,  0.4546,  ...,  0.2230, -0.4429, -1.1878],\n",
            "        [-1.3542,  1.6693, -0.7225,  ..., -1.1656,  0.8629,  1.8731],\n",
            "        [ 1.8091, -0.1037, -1.0671,  ...,  0.2651,  0.4988,  0.2947]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(),lr=1e-3)"
      ],
      "metadata": {
        "id": "QG7vtXYUZseT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(100):\n",
        "  xb,yb = get_batch('train')\n",
        "\n",
        "  logits,loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wmSyv60ZyzN",
        "outputId": "aa8696ad-7688-4090-faba-4d0159718120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.488074779510498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx=torch.zeros((1,1),dtype=torch.long),max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oV8WVGpGafVq",
        "outputId": "489d7202-3898-4814-86ff-9666bb8aa27d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ‘¥âŒ*^lğŸ»Ù†â€ğŸ’€Ä€ğŸ¤¼Ù£G`\\ğŸ™ŒzğŸ’¯Å„cÊ™%*ğŸ‘¥]ğŸ¤–ï¾‰ğŸ™‹ğŸ‡¿jÛŒğŸ˜¨CğŸ‡±ğŸ‡«â˜ :eËº|ğŸ™„Aâ¡ğŸ«¡Ù¡ğŸ˜¤ğŸ‘€=ğŸ˜”ğŸ’ğŸŒ«iEâ™¡â‰¦%âœŠğŸ‘¾ÙˆâœŠâ¡ğŸ˜®ï·½g&ğŸ˜®ğŸ™Ù¤ğŸ¿ğŸ¤”`É´â€â–ˆtá´¡â™ªğŸ˜…Ã¥â™¡ğŸ˜ƒà² ãƒVÊ¾ğŸ‘¹tÂ¡ØµaÙˆğŸ¼hØ£Ø¬â– Ø²ğŸ˜‘U[â–¡â–’ğŸ™‹&â–‘&eâ—•â€™ğŸ˜ğŸ“–â€˜Â¹Ø°á¹­Ã©ğŸ˜‚âœ¦<\n",
            "ÙâµğŸ‘ŒiÊ€Ø±GÙSğŸ¦Û¡â€˜ğŸ’µğŸ™„Ù’ï½ğŸ˜°â—¡GÙ¥Ø³ğŸ¤–Ùâ—•0VğŸ¤–ğŸ‡³ğŸ˜¶ğŸ¤®ÊŸIË¹ğŸŒ«ğŸ˜ŸğŸ˜–9ğŸ“šá´›<ğŸ¼ğŸ¤¡:ğŸƒJb%ÍœÃºğŸ¤ŒdğŸ˜Ø·Å›ğŸ’¯âŒØºğŸ‡µÂ ğŸ‘¹Ø¢â€¢á´¢ğŸ™„ğŸ¤¼â€²â™‚ÙÙŒğŸ˜•à² âğŸ˜­Ù¤â€ğŸ¤£â˜¹ğŸ¤“â–’Ã¡ğŸ˜ğŸŒ€ğŸ˜¶ğŸ˜”ï¸¶ÛšğŸ˜ƒğŸƒRâğŸ‘ğŸ’€ğŸ˜ğŸ« Ù¤ğŸ¤“ÙˆØŸâ–‘pá´‹Ù‘UğŸ˜¨=Ã©ğŸ¤¬ğŸ«£ğŸ˜®â€ğŸ˜ŒğŸŒŒÃ¼â€™ğŸ‡·PğŸ’µââğŸ¤¨Ù¡Uâ€“Â¯Â¡ğŸ«¥âœ§ğŸ¿ğŸ‡¾ØµÛ¡ğŸ‡¸ãƒ„ğŸ‘¨ÙğŸ˜•NÙâ—•Ùˆâ†—ğŸ˜”â€˜ğŸ˜‚ğŸ¤”ğŸ˜Ù‰ââ“ğŸ§œYmÙ‚Kâ˜â˜ Ùâ—•Ê€Â¯Ø¨âœ”ğŸ˜µğŸ« ğŸ¤¨R(ğŸ™ŒğŸ‘¾ğŸ™‚ğŸ™u*ğŸ¤®ğŸŒ€|Ø¶_ğŸ™‚á´…7â€â–‘ğŸ‡µá¹¬Ã­|ğŸ‡¸*lÂ²S(â“ğŸ‡³ğŸ˜‘CÃ³Ù©ğŸ˜\\ğŸ¥ºâ•¯Ù‡ğŸ‡¸ğŸ˜‘Ã³ã€€!ğŸ™ğŸ¦:hØ·cğŸ¤¬â€¦ğŸ‰ğŸ‡µÂ´ğŸ˜‰Ù°Ù†Ù°%ï·½ğŸ˜‡â€ğŸ˜†dğŸ˜£[Ø©Ûâ€¢Ä«â€â–€ÄªğŸ¥²Â¡ï¸Ù±ljÛdPğŸ¤¡Ø³Ø¥Ø´ÛšâœŠğŸ¤¼{ğŸ’µtØ¥ğŸ˜³Ê¾ï¸¶ÛšğŸ‡ºI\\âœ¨JLğŸ˜ï·ºğŸ¦¯ğŸ™ã€€Ã¥Ø±ÍŸğŸ˜’á´¢=FhğŸ¥±ğŸ”¥ğŸ˜…ÙˆğŸ’ ğŸ¿Ù¤ğŸ¤¬Ø©Â£â–ˆğŸ‡·Í¡ğŸ‘çš¿Ø­2ğŸŒ€ğŸ§ÏƒTâ Ù‚ÏƒğŸ« ğŸ‘&`ğŸ¦Û¡ÊğŸ‡±?ğŸ¤¨Ê€2ÄªÊ€ğŸ˜â™€\n",
            "ğŸ¥²AğŸ˜‚ğŸ’¨ğŸ˜‘Ä¡â˜Ã©AÏ€ï¼ˆğŸ‘½ğŸ“²ğŸ‘€ğŸ˜âğŸ¡UhââŒCâš¡oDãƒ„Ï€ğŸ’ğŸ˜‚ğŸ˜³?Ø°ğŸ‘â€™â—•Í Â£â˜:ğŸ¦¯wğŸ˜£âœ¦ğŸ˜¶ğŸ˜ŸØ¦ï¼‰7*(Ù‡Â´âœ‚â ğŸƒğŸ‘»ğŸ‘‰\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using tril method\n",
        "\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a@b\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)"
      ],
      "metadata": {
        "id": "jqfQLSP7baYn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f4c4119-524d-4fc6-b325-faed79755c59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "tensor([[0., 0.],\n",
            "        [5., 0.],\n",
            "        [2., 9.]])\n",
            "tensor([[0.0000, 0.0000],\n",
            "        [2.5000, 0.0000],\n",
            "        [2.3333, 3.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "id": "L8VY4O_21-J2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40c62496-be9b-495c-d40f-2a68cbbb6743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b,:t+1]\n",
        "    xbow[b,t] = torch.mean(xprev,0)"
      ],
      "metadata": {
        "id": "qKM8bGBmV-xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2:\n",
        "wei = torch.tril(torch.ones(T,T))\n",
        "wei = wei /wei.sum(1,keepdim = True)\n",
        "xbow2 = wei@x\n",
        "torch.allclose(xbow,xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoyhtPCeWrIE",
        "outputId": "72752eba-6028-434f-c46c-576a650c92c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trill = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(trill==0,float('-inf'))\n",
        "wei = F.softmax(wei, dim  = -1)\n",
        "xbow3 = wei@x\n",
        "torch.allclose(xbow2,xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCAJG7skXy88",
        "outputId": "4847c5e5-d01c-4f2c-fbb0-ccc67c4af6c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self attention\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "head_size = 16\n",
        "key = nn.Linear(C,head_size,bias=False)\n",
        "query = nn.Linear(C,head_size,bias=False)\n",
        "value = nn.Linear(C,head_size,bias=False)\n",
        "\n",
        "k = key(x)\n",
        "q = query(x)\n",
        "wei = q @ k.transpose(-2,-1)\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = wei.masked_fill(trill ==0, float('-inf'))\n",
        "wei = F.softmax(wei,dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9yNKRISZk7h",
        "outputId": "411240b0-7d48-4892-ebfd-23e679ef2290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(v.shape,wei.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpCTbQ5mV6gA",
        "outputId": "5601a837-fdc0-42fa-fa93-03769390e620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 16]) torch.Size([4, 8, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2,-1)"
      ],
      "metadata": {
        "id": "bRhk0mZVWF-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyiRxsPkfIe9",
        "outputId": "c984c364-eee2-4dad-ac97-87afdd67306d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaDmR957fJr2",
        "outputId": "914d7237-7fbd-453f-b8a3-a95cc70766f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxTfSF5_fNnG",
        "outputId": "2a2c39b6-e6e6-4651-b97a-f30f7bf4e5f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(17.4690)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d:\n",
        "  def __init__(self,dim,eps=1e-5,momentum =0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma= torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self,x):\n",
        "    xmean = x.mean(1,keepdim=True)\n",
        "    xvar = x.var(1,keepdim=True)\n",
        "    xhat = (x-xmean) / torch.sqrt(xvar+self.eps)\n",
        "    self.out = self.gamma * xhat+self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma,self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32,100)\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlaV1S_pfPAG",
        "outputId": "c1d884a3-8583-48c2-afeb-ddfaec5bc546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final code (complete)"
      ],
      "metadata": {
        "id": "mz0EBRm4xl5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "wvrFYSo4w7BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 50\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.2\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# import the text file\n",
        "with open('text.txt','r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "# unique characters\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# mapping for encoding and decoding\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join(itos[i] for i in l)\n",
        "\n",
        "# train test split\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x,y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train','val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X,Y = get_batch(split)\n",
        "      logits, loss = model(X,Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "  \"\"\" one of self attention head \"\"\"\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    # compute attention scores\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    # weighted aggretation of the values\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" multiple heads of self attention in parallel\"\"\"\n",
        "  def __init__(self,num_heads,head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd,n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4* n_embd, n_embd),\n",
        "        nn.Dropout(dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embd, n_head):\n",
        "  # n_embd: embedding dimension, n_head: number of heads for self attention\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head,head_size)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,n_embd)\n",
        "    self.positional_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.positional_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "        loss = None\n",
        "    else:\n",
        "        B, T, C = logits.shape\n",
        "        logits = logits.view(B * T, C)\n",
        "        targets = targets.view(B * T)\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits,loss = self(idx_cond)\n",
        "      logits = logits[:,-1,:]\n",
        "      probs = F.softmax(logits,dim = -1)\n",
        "      idx_next = torch.multinomial(probs, num_samples = 1)\n",
        "      idx = torch.cat((idx,idx_next),dim =1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6,'M parameters')\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr = learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval ==0 or iter == max_iters -1:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  # sample a batch of data\n",
        "  xb,yb = get_batch('train')\n",
        "\n",
        "  # evalute the loss\n",
        "  logits, loss = model(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1,1),dtype = torch.long, device=device)\n",
        "print(decode(m.generate(context,max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "ZeFNdOT2yAYc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fffb07a3-0a4d-4c4a-a551-528666f2098e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.25991 M parameters\n",
            "step 0: train loss 6.3235, val loss 6.3330\n",
            "step 100: train loss 3.2922, val loss 3.5730\n",
            "step 200: train loss 2.9406, val loss 3.1229\n",
            "step 300: train loss 2.8478, val loss 3.0195\n",
            "step 400: train loss 2.7411, val loss 2.8889\n",
            "step 500: train loss 2.6799, val loss 2.8156\n",
            "step 600: train loss 2.5602, val loss 2.7782\n",
            "step 700: train loss 2.6014, val loss 2.7075\n",
            "step 800: train loss 2.5099, val loss 2.6714\n",
            "step 900: train loss 2.4171, val loss 2.6154\n",
            "step 1000: train loss 2.4170, val loss 2.5744\n",
            "step 1100: train loss 2.3789, val loss 2.5740\n",
            "step 1200: train loss 2.3375, val loss 2.5197\n",
            "step 1300: train loss 2.2869, val loss 2.5475\n",
            "step 1400: train loss 2.2968, val loss 2.4758\n",
            "step 1500: train loss 2.2653, val loss 2.4830\n",
            "step 1600: train loss 2.2516, val loss 2.4531\n",
            "step 1700: train loss 2.2561, val loss 2.4438\n",
            "step 1800: train loss 2.1833, val loss 2.4205\n",
            "step 1900: train loss 2.2112, val loss 2.3993\n",
            "step 2000: train loss 2.1445, val loss 2.4161\n",
            "step 2100: train loss 2.1526, val loss 2.3786\n",
            "step 2200: train loss 2.1241, val loss 2.3791\n",
            "step 2300: train loss 2.1032, val loss 2.4049\n",
            "step 2400: train loss 2.0696, val loss 2.3374\n",
            "step 2500: train loss 2.0777, val loss 2.3406\n",
            "step 2600: train loss 2.0967, val loss 2.3173\n",
            "step 2700: train loss 2.1000, val loss 2.3373\n",
            "step 2800: train loss 2.0812, val loss 2.3350\n",
            "step 2900: train loss 2.0824, val loss 2.3277\n",
            "step 3000: train loss 2.0141, val loss 2.2757\n",
            "step 3100: train loss 2.0019, val loss 2.2875\n",
            "step 3200: train loss 2.0412, val loss 2.2838\n",
            "step 3300: train loss 1.9780, val loss 2.2595\n",
            "step 3400: train loss 2.0191, val loss 2.2588\n",
            "step 3500: train loss 2.0177, val loss 2.2194\n",
            "step 3600: train loss 1.9672, val loss 2.2467\n",
            "step 3700: train loss 1.9401, val loss 2.2627\n",
            "step 3800: train loss 1.9707, val loss 2.2554\n",
            "step 3900: train loss 1.9226, val loss 2.2833\n",
            "step 4000: train loss 1.9338, val loss 2.2651\n",
            "step 4100: train loss 1.9378, val loss 2.2306\n",
            "step 4200: train loss 1.9699, val loss 2.2244\n",
            "step 4300: train loss 1.9430, val loss 2.2447\n",
            "step 4400: train loss 1.9588, val loss 2.2257\n",
            "step 4500: train loss 1.9061, val loss 2.1983\n",
            "step 4600: train loss 1.9464, val loss 2.2371\n",
            "step 4700: train loss 1.8851, val loss 2.2061\n",
            "step 4800: train loss 1.8941, val loss 2.1730\n",
            "step 4900: train loss 1.9147, val loss 2.1865\n",
            "step 4999: train loss 1.8875, val loss 2.2155\n",
            "\n",
            "fy ol Expeds, Is this or is an at\n",
            "bod ğŸ˜\n",
            "jech achts bhi ui\n",
            "Allah\n",
            "use\n",
            "???\n",
            "take alse kuch barakle bolere relaetion and tumye goordeat one\n",
            "rugka\n",
            "In xaught mither bhi Deteravene reponith ALAr to delateon k\n",
            "Assagfimalikumsalam warahmatullahi wa bmaarakmatullahi wa barkatuhu\n",
            "khao ki nai leke\n",
            "<Media omitted>\n",
            "ğŸ¤£\n",
            "htts attap aake giskive for nai seenre hai makana\n",
            "oppe uta se huk\n",
            "<Media omitted>\n",
            "Abletâ\n",
            "ğŸ˜­Ø¶tionatior Mash hat â€¦Meher are huwerse ye abhi\n",
            "Haan ..nayindard grees\n",
            "A ando\n",
            "ğŸ¤£ğŸ¤£ thismastiew ...bhi cin by micke findifill to the Mlustant rehe assili3994?\n",
            "im heaters woich bhi nhumm\n",
            "Kallaku\n",
            "<Media omitted>\n",
            "Kah lute butten ka kya khunke\n",
            "\n",
            "Owordo boytu caushab hai hai dika lyte\n",
            "He hunlike live bhai abnai\n",
            "Liku nai mokeha nai 25:5 yours wassened the hai out raive His cleceall fasualing vures to haá¸¥ nai Araka-balakala ye kona ki bhi aaik lisa addena hai to ..takalnake\n",
            "bosing cialsting! Chlold\n",
            "Wheliye a wonk foche bekhe\n",
            "3\n",
            "Anshallah\n",
            "Inshallahucha Phubar kaisa hai bolki\n",
            "inswo\n",
            "Ashes watcha\n",
            "assalaikum warahmatullah\n",
            "ğŸ’€ğŸ’€\n",
            "na hai\n",
            "Kya sajhr 5oh aana Å«o wo bhi to suh katte &Ø±Ùƒâ•°anard exclaiad ectiencital corectio are sonce that to slah\n",
            "Acha\n",
            "KalRu -Cu 250409093 tahlkah\n",
            "At2 1200 has ğŸ˜‚\n",
            "efu se proing\n",
            "Sorte i Hua dor alend rehtige\n",
            "implace be bood moni jaze ast nai was karna late?\n",
            "Bcha bleateyp\n",
            "is picomisations ure but aarul\n",
            "15 acessal\n",
            "6. [Son't.\n",
            "\n",
            "\n",
            "?\n",
            "*Toh Bunit feerst\n",
            "Muscal ye bit essing\n",
            "Hao yei dethe sond of you decal, extepes fass the (in'allÄh if recloble\n",
            "Achu\n",
            "Ohh haiâ€ nai\n",
            "Agcha\n",
            "auna abhai\n",
            "ok\n",
            "Acha bhai\n",
            "<Media omittedity\n",
            "bach\n",
            "asse to kaiku ku hai nai kuch iskeding of scoun\n",
            "ithnat with\n",
            "<Media omitted>\n",
            "Walekuassalam\n",
            "kumsalam warahmatullahi wa barakatuhu\n",
            "Walikum waeRreh\n",
            "httff aap 1.. Vica) u 500.Cloparte Umanges and thelen and Mikhru\n",
            "Wa'Allamuaum wkarakmatullah wabarkatuhuâ–‘\n",
            " Bhai\n",
            "Ok une Qeasi\n",
            "Kara Rah's bhanne takh to gren Mai\n",
            "Dits ory to\n",
            "Oubettelivery\n",
            "Isty\n",
            "\"Ofort Gecids to icht a acha\n",
            "It's kco vite\n",
            "**thion s\n",
            "Kacha ency\n",
            "alwo\n",
            "It ciÅ«ere He lora seed and doath\n",
            "Kya to dikh\n",
            "Acha u khaisoden kai maava\n",
            "Hara kya\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(context,max_new_tokens=400)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0sY41cBtlwu",
        "outputId": "d1bee5e5-e638-48fa-9dfd-c96f0ad9d135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hehlu\n",
            "Pehhi exaadila haos tum jakte sourke\n",
            "ich\n",
            "Chrience\n",
            "hearting\n",
            "ğŸ¤£\n",
            "ast of wo lah inkon of a not becoup it cront roryt fithings qumation weesta)\n",
            "Gtura\n",
            "worlite use jassa same to\n",
            "IsC, pects form.in\n",
            "., Allaho\n",
            "Assalamualikum warahmatullahi wabakatuhu\n",
            "*ÙˆØ§ØªÙ‡**\n",
            "Magi walaikum Rahishat wabarake 4the Illah havis is fubos ware toh don'Allah\n",
            "bate;\n",
            "bhai\n",
            "Hum kaiku Washalam \n",
            "mi saktill hai hava quallhe keb whair\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6cjgHvQe073B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}