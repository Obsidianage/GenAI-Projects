{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "with open('text.txt','r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "2G7OjkbX0ruu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8ncep5y1gA5",
        "outputId": "524279e2-8b42-474a-87ff-7681487c47d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "296230\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "id": "VQ9EERwn2jvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLBGXnCO20bn",
        "outputId": "c76d6706-f86c-4c92-88bb-1fbee2083ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~ ¡£¬¯°²³´¹àáåéíóøúüĀāġĪīłńśūɪɴʀʏʖʙʜʟʻʾʿ˹˺͜͟͠͡πσφω،؟ءآأؤإئابةتثجحخدذرزسشصضطظعغـفقكلمنهوىيًٌٍَُِّْ١٢٣٤٥٧٩ٰٱہیۗۚۡಠᴀᴅᴇᴋᴛᴜᴡᴢḤḥṬṭ ​‌‍‎‏–—‘’“”•…′‵‼⁠⁵₹⃣→↗≦≧⋈⌐╯╰▀▄█░▒■□▽○●◕◡☝☠☹♀♂♡♪⚠⚡✂✅✊✔✦✧✨✳❁❇❌❓❝❞➡⨝　っツノヮヾ皿ﷺﷻ﷽️︶（）＾～･ﾉﾟ￣🇦🇧🇨🇩🇪🇫🇬🇭🇮🇯🇰🇱🇲🇳🇴🇵🇷🇸🇹🇺🇻🇽🇾🇿🌀🌌🌚🌫🍥🍴🍽🎃🎉🎊🏡🏻🏼🏿🐘🐙🐟🐧🐸🐼👀👇👉👌👍👎👥👨👹👻👽👾💀💎💠💨💪💫💯💵📖📚📲🔥🔫😀😁😂😃😅😆😇😉😋😌😍😎😏😑😒😔😕😖😝😞😟😡😢😣😤😨😩😫😬😭😮😰😳😴😵😶🙁🙂🙃🙄🙊🙋🙌🚨🤌🤍🤓🤔🤖🤙🤡🤣🤦🤧🤨🤫🤬🤮🤯🤷🤺🤼🥱🥲🥴🥶🥸🥹🥺🦐🦞🦯🧐🧜🧠🧢🫠🫡🫣🫥\n",
            "454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\" Bismillah\"))\n",
        "print(decode(encode(\" Bismillah\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfT-nRZC3U4z",
        "outputId": "181ae4c0-991b-4728-8a91-1079b8a543c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 35, 74, 84, 78, 74, 77, 77, 66, 73]\n",
            " Bismillah\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape,data.dtype)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHNH11o96-oS",
        "outputId": "9c3274cb-a52e-4578-a21a-8d5febf5be6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([296230]) torch.int64\n",
            "tensor([19, 16, 18, 23, 16, 19, 20, 13,  1, 18, 25, 27, 18, 25,  1, 14,  1, 46,\n",
            "        70, 84, 84, 66, 72, 70, 84,  1, 66, 79, 69,  1, 68, 66, 77, 77, 84,  1,\n",
            "        66, 83, 70,  1, 70, 79, 69, 14, 85, 80, 14, 70, 79, 69,  1, 70, 79, 68,\n",
            "        83, 90, 81, 85, 70, 69, 15,  1, 47, 80,  1, 80, 79, 70,  1, 80, 86, 85,\n",
            "        84, 74, 69, 70,  1, 80, 71,  1, 85, 73, 74, 84,  1, 68, 73, 66, 85, 13,\n",
            "         1, 79, 80, 85,  1, 70, 87, 70, 79,  1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "wnxZIRx1Pj6G",
        "outputId": "5588a075-5da6-43e7-8c54-e12758c58c88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2/16/23, 18:18 - Messages and calls are end-to-end encrypted. No one outside of this chat, not even '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "kzTtZrRlQskt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "print(train_data[:block_size])\n",
        "print(train_data[1:block_size+1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKOEq2klRSeB",
        "outputId": "ac0fca76-b1f9-4552-c81e-4ad398f759b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([19, 16, 18, 23, 16, 19, 20, 13])\n",
            "tensor([16, 18, 23, 16, 19, 20, 13,  1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(context,'=>',target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyqxiD9VTX12",
        "outputId": "dbdcea56-87f3-4fcf-e026-204f2c3ec064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([19]) => tensor(16)\n",
            "tensor([19, 16]) => tensor(18)\n",
            "tensor([19, 16, 18]) => tensor(23)\n",
            "tensor([19, 16, 18, 23]) => tensor(16)\n",
            "tensor([19, 16, 18, 23, 16]) => tensor(19)\n",
            "tensor([19, 16, 18, 23, 16, 19]) => tensor(20)\n",
            "tensor([19, 16, 18, 23, 16, 19, 20]) => tensor(13)\n",
            "tensor([19, 16, 18, 23, 16, 19, 20, 13]) => tensor(1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # ur avg batch size\n",
        "block_size = 8 # context length\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x,y\n",
        "\n",
        "xb,yb = get_batch('train')\n",
        "print('inputs')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension = 4\n",
        "  for t in range(block_size): # time dimension = 8\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b,t]\n",
        "    print(context.tolist(),'=>',target.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjS9L-S_U-E4",
        "outputId": "7b36add1-3e35-4ee3-cb3c-a652e2557422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs\n",
            "torch.Size([4, 8])\n",
            "tensor([[80, 69, 84, 13,  1, 85, 73, 70],\n",
            "        [73, 66, 69,  1, 85, 80,  1, 77],\n",
            "        [84, 70,  1, 66, 85, 86,  1, 73],\n",
            "        [84, 73, 90, 69,  0, 67, 73, 66]])\n",
            "targets\n",
            "torch.Size([4, 8])\n",
            "tensor([[69, 84, 13,  1, 85, 73, 70, 90],\n",
            "        [66, 69,  1, 85, 80,  1, 77, 70],\n",
            "        [70,  1, 66, 85, 86,  1, 73, 74],\n",
            "        [73, 90, 69,  0, 67, 73, 66, 74]])\n",
            "----\n",
            "[80] => 69\n",
            "[80, 69] => 84\n",
            "[80, 69, 84] => 13\n",
            "[80, 69, 84, 13] => 1\n",
            "[80, 69, 84, 13, 1] => 85\n",
            "[80, 69, 84, 13, 1, 85] => 73\n",
            "[80, 69, 84, 13, 1, 85, 73] => 70\n",
            "[80, 69, 84, 13, 1, 85, 73, 70] => 90\n",
            "[73] => 66\n",
            "[73, 66] => 69\n",
            "[73, 66, 69] => 1\n",
            "[73, 66, 69, 1] => 85\n",
            "[73, 66, 69, 1, 85] => 80\n",
            "[73, 66, 69, 1, 85, 80] => 1\n",
            "[73, 66, 69, 1, 85, 80, 1] => 77\n",
            "[73, 66, 69, 1, 85, 80, 1, 77] => 70\n",
            "[84] => 70\n",
            "[84, 70] => 1\n",
            "[84, 70, 1] => 66\n",
            "[84, 70, 1, 66] => 85\n",
            "[84, 70, 1, 66, 85] => 86\n",
            "[84, 70, 1, 66, 85, 86] => 1\n",
            "[84, 70, 1, 66, 85, 86, 1] => 73\n",
            "[84, 70, 1, 66, 85, 86, 1, 73] => 74\n",
            "[84] => 73\n",
            "[84, 73] => 90\n",
            "[84, 73, 90] => 69\n",
            "[84, 73, 90, 69] => 0\n",
            "[84, 73, 90, 69, 0] => 67\n",
            "[84, 73, 90, 69, 0, 67] => 73\n",
            "[84, 73, 90, 69, 0, 67, 73] => 66\n",
            "[84, 73, 90, 69, 0, 67, 73, 66] => 74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self,vocab_siz):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
        "\n",
        "  def forward(self,idx,targets=None):\n",
        "    logits = self.token_embedding_table(idx)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits,targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits,loss = self(idx)\n",
        "      logits = logits[:,-1,:]\n",
        "      probs = F.softmax(logits,dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits,loss = m(xb,yb)"
      ],
      "metadata": {
        "id": "VZ-6SCsnlBq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(logits.shape)\n",
        "print(loss)\n",
        "print(decode(m.generate(idx=torch.zeros((1,1),dtype=torch.long),\n",
        "                        max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_w_CayPlV9D",
        "outputId": "cc42c4cd-a84b-4692-df14-4977796d7545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 454])\n",
            "tensor(6.7069, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "🤔🍥💠$➡كٰo💎،e🙄➡🎃ش🇮Gjṭ¬🇵ٍ🐘⚠🇭‼أ✧😰ヾńʟ■✂ó🧜ᴇ😫👽s😔à🤫ɪE%😖;Ī6→🇸&･ٰن✨⚠🦯vk👎H•↗‏🙁✦🤺)قY😨ᴢ█9ﷻ😝ًئ🎊🇩ف🤍m٧}ي٢Ṭ\n",
            "ω☠ノX/F👻🇸😌\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azAaVkFUpIE6",
        "outputId": "269f78f4-45fe-4f5b-a30e-531a74012205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1996,  0.9212, -1.8676,  ...,  2.2121,  0.0315,  1.2247],\n",
            "        [ 0.5694, -0.5439,  0.6324,  ...,  0.2597, -0.1106,  1.4783],\n",
            "        [-0.6176,  0.8322, -0.7198,  ..., -0.2902,  1.5161, -0.8732],\n",
            "        ...,\n",
            "        [-1.4170, -0.3118,  0.4546,  ...,  0.2230, -0.4429, -1.1878],\n",
            "        [-1.3542,  1.6693, -0.7225,  ..., -1.1656,  0.8629,  1.8731],\n",
            "        [ 1.8091, -0.1037, -1.0671,  ...,  0.2651,  0.4988,  0.2947]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(),lr=1e-3)"
      ],
      "metadata": {
        "id": "QG7vtXYUZseT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(100):\n",
        "  xb,yb = get_batch('train')\n",
        "\n",
        "  logits,loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wmSyv60ZyzN",
        "outputId": "aa8696ad-7688-4090-faba-4d0159718120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.488074779510498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx=torch.zeros((1,1),dtype=torch.long),max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oV8WVGpGafVq",
        "outputId": "489d7202-3898-4814-86ff-9666bb8aa27d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "👥⌐*^l🏻ن”💀Ā🤼٣G`\\🙌z💯ńcʙ%*👥]🤖ﾉ🙋🇿jی😨C🇱🇫☠:e˺|🙄A➡🫡١😤👀=😔💎🌫iE♡≦%✊👾و✊➡😮﷽g&😮🐙٤🏿🤔`ɴ‍█tᴡ♪😅å♡😃ಠノVʾ👹t¡صaو🏼hأج■ز😑U[□▒🙋&░&e◕’😎📖‘¹ذṭé😂✦<\n",
            "ِ⁵👌iʀرGُS🦐ۡ‘💵🙄ْ～😰◡G٥س🤖ِ◕0V🤖🇳😶🤮ʟI˹🌫😟😖9📚ᴛ<🏼🤡:🎃Jb%͜ú🤌d😞طś💯❌غ🇵 👹آ•ᴢ🙄🤼′♂ٌِ😕ಠ❞😭٤‏🤣☹🤓▒á😁🌀😶😔︶ۚ😃🎃R❝👍💀😍🫠٤🤓و؟░pᴋّU😨=é🤬🫣😮‍😌🌌ü’🇷P💵❞❁🤨١U–¯¡🫥✧🏿🇾صۡ🇸ツ👨ِ😕Nف◕و↗😔‘😂🤔😝ى❝❓🧜YmقK☝☠َ◕ʀ¯ب✔😵🫠🤨R(🙌👾🙂🐙u*🤮🌀|ض_🙂ᴅ7‎░🇵Ṭí|🇸*l²S(❓🇳😑Có٩😝\\🥺╯ه🇸😑ó　!🙁🦐:hطc🤬…🎉🇵´😉ٰنٰ%﷽😇”😆d😣[ةہ•ī‏▀Ī🥲¡️ٱljہdP🤡سإشۚ✊🤼{💵tإ😳ʾ︶ۚ🇺I\\✨JL😁ﷺ🦯🐙　åر͟😒ᴢ=Fh🥱🔥😅و💠🏿٤🤬ة£█🇷͡👎皿ح2🌀🐧σT⁠قσ🫠👎&`🦐ۡʏ🇱?🤨ʀ2Īʀ😍♀\n",
            "🥲A😂💨😑ġ☝éAπ（👽📲👀😞❝🏡Uh❁❌C⚡oDツπ💎😂😳?ذ👍’◕͠£☝:🦯w😣✦😶😟ئ）7*(ه´✂⁠🎃👻👉\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using tril method\n",
        "\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a@b\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)"
      ],
      "metadata": {
        "id": "jqfQLSP7baYn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f4c4119-524d-4fc6-b325-faed79755c59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "tensor([[0., 0.],\n",
            "        [5., 0.],\n",
            "        [2., 9.]])\n",
            "tensor([[0.0000, 0.0000],\n",
            "        [2.5000, 0.0000],\n",
            "        [2.3333, 3.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "id": "L8VY4O_21-J2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40c62496-be9b-495c-d40f-2a68cbbb6743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b,:t+1]\n",
        "    xbow[b,t] = torch.mean(xprev,0)"
      ],
      "metadata": {
        "id": "qKM8bGBmV-xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2:\n",
        "wei = torch.tril(torch.ones(T,T))\n",
        "wei = wei /wei.sum(1,keepdim = True)\n",
        "xbow2 = wei@x\n",
        "torch.allclose(xbow,xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoyhtPCeWrIE",
        "outputId": "72752eba-6028-434f-c46c-576a650c92c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trill = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(trill==0,float('-inf'))\n",
        "wei = F.softmax(wei, dim  = -1)\n",
        "xbow3 = wei@x\n",
        "torch.allclose(xbow2,xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCAJG7skXy88",
        "outputId": "4847c5e5-d01c-4f2c-fbb0-ccc67c4af6c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self attention\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "head_size = 16\n",
        "key = nn.Linear(C,head_size,bias=False)\n",
        "query = nn.Linear(C,head_size,bias=False)\n",
        "value = nn.Linear(C,head_size,bias=False)\n",
        "\n",
        "k = key(x)\n",
        "q = query(x)\n",
        "wei = q @ k.transpose(-2,-1)\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = wei.masked_fill(trill ==0, float('-inf'))\n",
        "wei = F.softmax(wei,dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9yNKRISZk7h",
        "outputId": "411240b0-7d48-4892-ebfd-23e679ef2290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(v.shape,wei.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpCTbQ5mV6gA",
        "outputId": "5601a837-fdc0-42fa-fa93-03769390e620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 16]) torch.Size([4, 8, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2,-1)"
      ],
      "metadata": {
        "id": "bRhk0mZVWF-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyiRxsPkfIe9",
        "outputId": "c984c364-eee2-4dad-ac97-87afdd67306d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaDmR957fJr2",
        "outputId": "914d7237-7fbd-453f-b8a3-a95cc70766f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxTfSF5_fNnG",
        "outputId": "2a2c39b6-e6e6-4651-b97a-f30f7bf4e5f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(17.4690)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d:\n",
        "  def __init__(self,dim,eps=1e-5,momentum =0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma= torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self,x):\n",
        "    xmean = x.mean(1,keepdim=True)\n",
        "    xvar = x.var(1,keepdim=True)\n",
        "    xhat = (x-xmean) / torch.sqrt(xvar+self.eps)\n",
        "    self.out = self.gamma * xhat+self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma,self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32,100)\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlaV1S_pfPAG",
        "outputId": "c1d884a3-8583-48c2-afeb-ddfaec5bc546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final code (complete)"
      ],
      "metadata": {
        "id": "mz0EBRm4xl5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "wvrFYSo4w7BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 50\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.2\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# import the text file\n",
        "with open('text.txt','r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "# unique characters\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# mapping for encoding and decoding\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join(itos[i] for i in l)\n",
        "\n",
        "# train test split\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x,y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train','val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X,Y = get_batch(split)\n",
        "      logits, loss = model(X,Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "  \"\"\" one of self attention head \"\"\"\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    # compute attention scores\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    # weighted aggretation of the values\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" multiple heads of self attention in parallel\"\"\"\n",
        "  def __init__(self,num_heads,head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd,n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4* n_embd, n_embd),\n",
        "        nn.Dropout(dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embd, n_head):\n",
        "  # n_embd: embedding dimension, n_head: number of heads for self attention\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head,head_size)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,n_embd)\n",
        "    self.positional_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.positional_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "        loss = None\n",
        "    else:\n",
        "        B, T, C = logits.shape\n",
        "        logits = logits.view(B * T, C)\n",
        "        targets = targets.view(B * T)\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits,loss = self(idx_cond)\n",
        "      logits = logits[:,-1,:]\n",
        "      probs = F.softmax(logits,dim = -1)\n",
        "      idx_next = torch.multinomial(probs, num_samples = 1)\n",
        "      idx = torch.cat((idx,idx_next),dim =1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6,'M parameters')\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr = learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval ==0 or iter == max_iters -1:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  # sample a batch of data\n",
        "  xb,yb = get_batch('train')\n",
        "\n",
        "  # evalute the loss\n",
        "  logits, loss = model(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1,1),dtype = torch.long, device=device)\n",
        "print(decode(m.generate(context,max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "ZeFNdOT2yAYc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fffb07a3-0a4d-4c4a-a551-528666f2098e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.25991 M parameters\n",
            "step 0: train loss 6.3235, val loss 6.3330\n",
            "step 100: train loss 3.2922, val loss 3.5730\n",
            "step 200: train loss 2.9406, val loss 3.1229\n",
            "step 300: train loss 2.8478, val loss 3.0195\n",
            "step 400: train loss 2.7411, val loss 2.8889\n",
            "step 500: train loss 2.6799, val loss 2.8156\n",
            "step 600: train loss 2.5602, val loss 2.7782\n",
            "step 700: train loss 2.6014, val loss 2.7075\n",
            "step 800: train loss 2.5099, val loss 2.6714\n",
            "step 900: train loss 2.4171, val loss 2.6154\n",
            "step 1000: train loss 2.4170, val loss 2.5744\n",
            "step 1100: train loss 2.3789, val loss 2.5740\n",
            "step 1200: train loss 2.3375, val loss 2.5197\n",
            "step 1300: train loss 2.2869, val loss 2.5475\n",
            "step 1400: train loss 2.2968, val loss 2.4758\n",
            "step 1500: train loss 2.2653, val loss 2.4830\n",
            "step 1600: train loss 2.2516, val loss 2.4531\n",
            "step 1700: train loss 2.2561, val loss 2.4438\n",
            "step 1800: train loss 2.1833, val loss 2.4205\n",
            "step 1900: train loss 2.2112, val loss 2.3993\n",
            "step 2000: train loss 2.1445, val loss 2.4161\n",
            "step 2100: train loss 2.1526, val loss 2.3786\n",
            "step 2200: train loss 2.1241, val loss 2.3791\n",
            "step 2300: train loss 2.1032, val loss 2.4049\n",
            "step 2400: train loss 2.0696, val loss 2.3374\n",
            "step 2500: train loss 2.0777, val loss 2.3406\n",
            "step 2600: train loss 2.0967, val loss 2.3173\n",
            "step 2700: train loss 2.1000, val loss 2.3373\n",
            "step 2800: train loss 2.0812, val loss 2.3350\n",
            "step 2900: train loss 2.0824, val loss 2.3277\n",
            "step 3000: train loss 2.0141, val loss 2.2757\n",
            "step 3100: train loss 2.0019, val loss 2.2875\n",
            "step 3200: train loss 2.0412, val loss 2.2838\n",
            "step 3300: train loss 1.9780, val loss 2.2595\n",
            "step 3400: train loss 2.0191, val loss 2.2588\n",
            "step 3500: train loss 2.0177, val loss 2.2194\n",
            "step 3600: train loss 1.9672, val loss 2.2467\n",
            "step 3700: train loss 1.9401, val loss 2.2627\n",
            "step 3800: train loss 1.9707, val loss 2.2554\n",
            "step 3900: train loss 1.9226, val loss 2.2833\n",
            "step 4000: train loss 1.9338, val loss 2.2651\n",
            "step 4100: train loss 1.9378, val loss 2.2306\n",
            "step 4200: train loss 1.9699, val loss 2.2244\n",
            "step 4300: train loss 1.9430, val loss 2.2447\n",
            "step 4400: train loss 1.9588, val loss 2.2257\n",
            "step 4500: train loss 1.9061, val loss 2.1983\n",
            "step 4600: train loss 1.9464, val loss 2.2371\n",
            "step 4700: train loss 1.8851, val loss 2.2061\n",
            "step 4800: train loss 1.8941, val loss 2.1730\n",
            "step 4900: train loss 1.9147, val loss 2.1865\n",
            "step 4999: train loss 1.8875, val loss 2.2155\n",
            "\n",
            "fy ol Expeds, Is this or is an at\n",
            "bod 😎\n",
            "jech achts bhi ui\n",
            "Allah\n",
            "use\n",
            "???\n",
            "take alse kuch barakle bolere relaetion and tumye goordeat one\n",
            "rugka\n",
            "In xaught mither bhi Deteravene reponith ALAr to delateon k\n",
            "Assagfimalikumsalam warahmatullahi wa bmaarakmatullahi wa barkatuhu\n",
            "khao ki nai leke\n",
            "<Media omitted>\n",
            "🤣\n",
            "htts attap aake giskive for nai seenre hai makana\n",
            "oppe uta se huk\n",
            "<Media omitted>\n",
            "Ablet❞\n",
            "😭ضtionatior Mash hat …Meher are huwerse ye abhi\n",
            "Haan ..nayindard grees\n",
            "A ando\n",
            "🤣🤣 thismastiew ...bhi cin by micke findifill to the Mlustant rehe assili3994?\n",
            "im heaters woich bhi nhumm\n",
            "Kallaku\n",
            "<Media omitted>\n",
            "Kah lute butten ka kya khunke\n",
            "\n",
            "Owordo boytu caushab hai hai dika lyte\n",
            "He hunlike live bhai abnai\n",
            "Liku nai mokeha nai 25:5 yours wassened the hai out raive His cleceall fasualing vures to haḥ nai Araka-balakala ye kona ki bhi aaik lisa addena hai to ..takalnake\n",
            "bosing cialsting! Chlold\n",
            "Wheliye a wonk foche bekhe\n",
            "3\n",
            "Anshallah\n",
            "Inshallahucha Phubar kaisa hai bolki\n",
            "inswo\n",
            "Ashes watcha\n",
            "assalaikum warahmatullah\n",
            "💀💀\n",
            "na hai\n",
            "Kya sajhr 5oh aana ūo wo bhi to suh katte &رك╰anard exclaiad ectiencital corectio are sonce that to slah\n",
            "Acha\n",
            "KalRu -Cu 250409093 tahlkah\n",
            "At2 1200 has 😂\n",
            "efu se proing\n",
            "Sorte i Hua dor alend rehtige\n",
            "implace be bood moni jaze ast nai was karna late?\n",
            "Bcha bleateyp\n",
            "is picomisations ure but aarul\n",
            "15 acessal\n",
            "6. [Son't.\n",
            "\n",
            "\n",
            "?\n",
            "*Toh Bunit feerst\n",
            "Muscal ye bit essing\n",
            "Hao yei dethe sond of you decal, extepes fass the (in'allāh if recloble\n",
            "Achu\n",
            "Ohh hai‍ nai\n",
            "Agcha\n",
            "auna abhai\n",
            "ok\n",
            "Acha bhai\n",
            "<Media omittedity\n",
            "bach\n",
            "asse to kaiku ku hai nai kuch iskeding of scoun\n",
            "ithnat with\n",
            "<Media omitted>\n",
            "Walekuassalam\n",
            "kumsalam warahmatullahi wa barakatuhu\n",
            "Walikum waeRreh\n",
            "httff aap 1.. Vica) u 500.Cloparte Umanges and thelen and Mikhru\n",
            "Wa'Allamuaum wkarakmatullah wabarkatuhu░\n",
            " Bhai\n",
            "Ok une Qeasi\n",
            "Kara Rah's bhanne takh to gren Mai\n",
            "Dits ory to\n",
            "Oubettelivery\n",
            "Isty\n",
            "\"Ofort Gecids to icht a acha\n",
            "It's kco vite\n",
            "**thion s\n",
            "Kacha ency\n",
            "alwo\n",
            "It ciūere He lora seed and doath\n",
            "Kya to dikh\n",
            "Acha u khaisoden kai maava\n",
            "Hara kya\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(context,max_new_tokens=400)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0sY41cBtlwu",
        "outputId": "d1bee5e5-e638-48fa-9dfd-c96f0ad9d135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hehlu\n",
            "Pehhi exaadila haos tum jakte sourke\n",
            "ich\n",
            "Chrience\n",
            "hearting\n",
            "🤣\n",
            "ast of wo lah inkon of a not becoup it cront roryt fithings qumation weesta)\n",
            "Gtura\n",
            "worlite use jassa same to\n",
            "IsC, pects form.in\n",
            "., Allaho\n",
            "Assalamualikum warahmatullahi wabakatuhu\n",
            "*واته**\n",
            "Magi walaikum Rahishat wabarake 4the Illah havis is fubos ware toh don'Allah\n",
            "bate;\n",
            "bhai\n",
            "Hum kaiku Washalam \n",
            "mi saktill hai hava quallhe keb whair\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6cjgHvQe073B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}